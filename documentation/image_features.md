# A Guide to Extracting the Image Features for the different Transformers

The multimodal Transformers require a pre-processing step 
where we extract the regions of interest from the images using a Faster R-CNN object detection model, which then serve as image input analog to word tokens.

This makes using the models slightly more challenging than e.g., a CNN-model that can directly use the image files as input.
However, the main challenge is figuring out what detection model is needed and how to run it.
In this guide, we will point to the right repositories for the models and give the used parameters used for feature extraction.

Be aware that pre-processing large numbers of images takes time (a few days depending on the hardware for a million images).
However, for many multimodal datasets, already extracted image features are available for download. 

We can group the currently supported Transformers in the following groups. 
Each group uses the same image features, so the processed features can be used for every Transformer in the group:

1. OSCAR, UNITER, VILLA
1. M3P


## OSCAR, UNITER, VILLA
### Generate
All use the bottom-up attention model from https://github.com/peteanderson80/bottom-up-attention with the default parameters:
min boxes 10, max boxes 100, confidence threshold 0.2 and the ResNet-101 backbone.

However, the above code is based on Caffe and Python 2 which might be challenging to install (to put it lightly).
Instead, we recommend the Pytorch reimplementation with migrated Caffe models here: https://github.com/MILVLG/bottom-up-attention.pytorch   
The code is based on detectron2 and is being updated (at time of writing) for newer Pytorch and detectron2 versions.
Simply follow their [installation guide](https://github.com/MILVLG/bottom-up-attention.pytorch#prerequisites)
and [use their feature extraction code](https://github.com/MILVLG/bottom-up-attention.pytorch#feature-extraction).   
(Alternative by us untested repository: https://github.com/airsplay/py-bottom-up-attention. 
See discussion here https://github.com/microsoft/Oscar/issues/49)

### Download
You can download the already extracted features for MSCOCO and Flickr30 
[here for MSCOCO](https://storage.googleapis.com/up-down-attention/trainval.zip) (credit to https://github.com/peteanderson80/bottom-up-attention#pretrained-features) 
and [here for Flickr30k](https://drive.google.com/uc?export=download&id=11OD_qq7ITBarJwWZfi0bWIRw3HPEaHwE) 
(credits to BAN: https://github.com/jnhwkim/ban-vqa/blob/master/tools/download_flickr.sh) 
in the .tsv format.

We host the pre-processed features for the Conceptual Captions dev set [here](https://public.ukp.informatik.tu-darmstadt.de/reimers/mmt-retrieval/datasets/cc_devtest.zip).

OSCAR also provides downloadable features for MSCOCO, VQA, GQA and NLVR2 [here](https://github.com/microsoft/Oscar/blob/master/DOWNLOAD.md#datasets).



### Loading Features

We support the .tsv format generated by the original Caffe repository and the .npz files generated by the Pytorch reimplementation.
We also support specifically the OSCAR-formatted files.
````python
model.image_dict.load_features_folder(feature_folder, extension="npz") # for .npz
model.image_dict.load_obj_tsv(tsv_file) # for .tsv
model.image_dict.load_oscar_format_image_features(feature_file) # for Oscar's .pt feature file
````


## M3P
### Generate

M3P extracts the features using [this script](https://github.com/facebookresearch/mmf/blob/master/tools/scripts/features/extract_features_vmb.py) with the
ResNet-101 backbone (that is ``model_name='X-101'``) and otherwise the default parameters.
Note that the script does not use the mmf package except for downloading the model weights and the config - if you download these files manually, you can comment the import statement out and do not have to install mmf.

The script uses [vqa-maskrcnn-benchmark](https://gitlab.com/vedanuj/vqa-maskrcnn-benchmark) for the detection model implementation and so the repository must be installed.

### Download
There are currently no already extracted features available for download, as far as we are aware.

### Loading Features
We support the .pt files generated by the above script. The image features for each image are placed in separate files.
````python
model.image_dict.load_features_folder(feature_folder, extension="pt") # for .pt
````

## Installation Troubleshooting
We list some of the problems that we encountered during installation of the above repositories.

### You need CUDA
You need the full CUDA toolkit with compiler to install the needed repositories.
Conda's cudatoolkit is not enough because it lacks the compilation support.


### Windows and CUDA Compilation Error
If you get a compilation error for CUDA during installation of one of the above packages for ROIAlign_cuda.cu/ ROIPool_cuda.cu or similar,
maybe check step 12 [in this issue](https://github.com/facebookresearch/maskrcnn-benchmark/issues/1042) if it can help. 
It did for me.